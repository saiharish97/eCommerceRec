# -*- coding: utf-8 -*-
"""eCommerce_ETL_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POUz3Jnhe5QEZ4n6tuDvwQoC3Ktknnr9
"""

from google.colab import drive
drive.mount('/content/drive/',force_remount=True)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf /content/drive/Shareddrives/FourYottaBytes_DA231o/spark-3.0.3-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop2.7"
import findspark
findspark.init()
findspark.find()
from pyspark.sql import SparkSession
spark = SparkSession.builder\
         .master("local[*]")\
         .appName("Colab")\
         .config('spark.ui.port', '4050')\
         .config("spark.executor.memory", '8g')\
         .config("spark.driver.memory", '16g')\
         .getOrCreate()
spark

spark.sparkContext._conf.getAll()

### IMPORTS ###
import math
import pandas as pd
from typing import Any
from pyspark.sql.functions import col,collect_list,udf,avg,stddev_pop,split,to_date
from pyspark.sql.types import IntegerType, StructType, StructField, StringType, FloatType,TimestampType
import statistics as st

### RAW-DATA-ETL-PREPROCESSING ###
class RawDataProcessor():
    """
      This is a class for doing the raw data etl for event logs.
        
      Attributes:
          df (Dataframe): The dataframe that holds the historical event logs.
    """
    df=Any

    def __init__(self, input,output):
        """
          The constructor for RawDataProcessor class.
    
          Parameters:
            input (String): The path to input logs.
            output (String): The path to save the processed logs.
        """
        self.df=self.extractFromCsv(input)
        self.preprocessData(self.df)
        self.loadAsParquet(self.df,output)

    def extractFromCSV(input_file):
        """
          This method reads the given csv into a dataframe.
      
          Parameters:
            input_file (String): The path to input logs.
        
          Returns:
            The dataframe containing raw logs.
      
        """
        df=spark.read.option("header","true").csv(input_file)
        return df

    def preprocessData(df):
      """
        This method corrects the schema and does some transformations.

        The schema is corrected and null values are removed.
    
        Parameters:
          df (Dataframe): The dataframe containing raw logs.
      
        Returns:
          The dataframe after processing the logs.
    
      """
      dfSchemaChange=df.withColumn("event_time",col("event_time").cast(TimestampType()))\
            .withColumn("price",col("price").cast(FloatType()))\
            .withColumn("date",to_date(col("event_time")))\
            .select("event_time","date","event_type","product_id","category_id","category_code","brand","price","user_id","user_session").na.drop(how="any")  
      return dfSchemaChange

    def loadAsParquet(df,output_path):
      """
        This method loads the processed dataframe as a parquet file at the specified location

        The schema is corrected and null values are removed.
    
        Parameters:
          df (Dataframe): The dataframe containing processed logs.
          output_path (String): The path to save the processed logs.

        Returns:
          None
    
      """
      tag="schema_verified/"
      df.write.mode('append').partitionBy("date").parquet(output_path+tag)

class UserProfileGenerator():
    """
      This is a class for profile generation on users.
        
      Attributes:
          total_df (Dataframe): The dataframe that holds the historical data of the users.
          save_flag (Boolean): True if save is required.
          save_tag (String): A tag which is given to the save location.
          category_flag (Boolean): True if the profile is to be generated at category level.
    """
    total_df = Any
    save_flag = False
    save_tag = "new"
    category_flag = False

    def __init__(self, tdf, save_flag, save_tag, category_flag):
        """
          The constructor for UserProfileGenerator class.
    
          Parameters:
            tdf (Dataframe): The dataframe consisting user historical data.
            save_flag (Boolean): True if save is required.  Default is False.
            save_tag (String): A tag which is given to the save location.
            category_flag (Boolean): True if the profile is to be generated at category level.
        """
        self.total_df = tdf
        self.save_flag = save_flag
        self.save_tag = save_tag
        self.category_flag = category_flag
        self.user_history_generator()


    def user_history_generator(self):
        """
          This method generates the user profile by generating some stats.
        
          The stats generated are: [event_count,avg_price,stddev_price,event_price_history,product_purchase_history]
          The dataframe is saved in parquet format.
        
          Parameters:
            None
        
          Returns:
            None
      
        """

        if self.category_flag == False:
            self.total_df.cache()
            countdf = (
                self.total_df.select("user_id", "event_type")
                .groupBy("user_id", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            avgdf = (
                self.total_df.select("user_id", "event_type", "price")
                .groupBy("user_id", "event_type")
                .avg("price")
                .withColumn("avg_event_price", col("avg(price)"))
                .drop("avg(price)")
            )
            event_history = (
                self.total_df.select("user_id", "event_type", "price")
                .groupBy("user_id", "event_type")
                .agg(
                    stddev_pop("price").alias("stddev"),    
                    collect_list("price").alias("event_history"),
                )
            )
            user_product_history = (
                self.total_df.filter(col("event_type") == "purchase")
                .select("user_id", "product_id")
                .groupBy("user_id")
                .agg(collect_list("product_id").alias("product_history"))
            )
            user_history_df = (
                countdf.join(avgdf, ["user_id", "event_type"])
                .join(event_history, ["user_id", "event_type"])
                .join(user_product_history, "user_id")
            )
            if self.save_flag:
                user_history_df.write.option("overwrite", "true").parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_store_demo/"
                    + self.save_tag
                    + "/"
                )
            self.total_df.unpersist()
        else:
            ct_total_df = self.total_df.withColumn(
                "main_category", split(col("category_code"), "\.")[0]
            )
            ct_total_df.cache()
            countdf = (
                ct_total_df.select("user_id", "main_category", "event_type")
                .groupBy("user_id", "main_category", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            avgdf = (
                ct_total_df.select("user_id", "main_category", "event_type", "price")
                .groupBy("user_id", "main_category", "event_type")
                .avg("price")
                .withColumn("avg_event_price", col("avg(price)"))
                .drop("avg(price)")
            )
            event_history = (
                ct_total_df.select("user_id", "main_category", "event_type", "price")
                .groupBy("user_id", "main_category", "event_type")
                .agg(
                    stddev_pop("price").alias("stddev"),
                    collect_list("price").alias("event_history"),
                )
            )
            user_product_history = (
                ct_total_df.filter(col("event_type") == "purchase")
                .select("user_id", "main_category", "product_id")
                .groupBy("user_id", "main_category")
                .agg(collect_list("product_id").alias("product_history"))
            )
            user_history_df = (
                countdf.join(avgdf, ["user_id", "main_category", "event_type"])
                .join(event_history, ["user_id", "main_category", "event_type"])
                .join(user_product_history, ["user_id", "main_category"])
            )
            if self.save_flag:
                user_history_df.write.option("overwrite", "true").partitionBy(
                    "main_category"
                ).parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_by_category_store_demo/"
                    + self.save_tag
                    + "/"
                )
            ct_total_df.unpersist()

class CatalogGenerator():
    """
      This is a class for product catalog generation.
        
      Attributes:
          total_df (Dataframe): The dataframe that holds the historical event data.
          save_flag (Boolean): True if save is required.
          save_tag (String): A tag which is given to the save location.
          category_flag (Boolean): True if the catalog is to be generated at category level.
    """
    total_df = Any
    save_flag = False
    save_tag = "new"
    category_flag = False

    def __init__(self, tdf, save_flag, save_tag, category_flag):
        """
          The constructor for CatalogGenerator class.
    
          Parameters:
            tdf (Dataframe): The dataframe consisting user historical data.
            save_flag (Boolean): True if save is required.  Default is False.
            save_tag (String): A tag which is given to the save location.
            category_flag (Boolean): True if the catalog is to be generated at category level.
        """
        self.total_df = tdf
        self.save_flag = save_flag
        self.save_tag = save_tag
        self.category_flag = category_flag
        self.product_catalog_generator()

    def product_catalog_generator(self):
        """
          This method generates the product catalog by generating some stats.
        
          The catalog generated contains the following fields for each product for each event type: [event_count,avg_price]
          The dataframe is saved in parquet format.
        
          Parameters:
            None
        
          Returns:
            None
        """
        if self.category_flag == False:
            self.total_df.cache()
            productcountdf = (
                self.total_df.select("product_id", "event_type")
                .groupBy("product_id", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            productavgdf = (
                self.total_df.select("product_id", "price")
                .groupBy("product_id")
                .avg("price")
                .withColumn("avg_price", col("avg(price)"))
                .drop("avg(price)")
            )
            productmetadf = self.total_df.select(
                "product_id", "category_code", "brand"
            ).distinct()
            product_catalog_df = productcountdf.join(productavgdf, "product_id").join(
                productmetadf, "product_id"
            )
            if self.save_flag:
                product_catalog_df.write.option("overwrite", "true").parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_store/"
                    + self.save_tag
                    + "/"
                )
            self.total_df.unpersist()
        else:
            ct_total_df = self.total_df.withColumn(
                "main_category", split(col("category_code"), "\.")[0]
            )
            ct_total_df.cache()
            productcountdf = (
                ct_total_df.select("product_id", "main_category", "event_type")
                .groupBy("product_id", "main_category", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            productavgdf = (
                ct_total_df.select("product_id", "main_category", "price")
                .groupBy("product_id", "main_category")
                .avg("price")
                .withColumn("avg_price", col("avg(price)"))
                .drop("avg(price)")
            )
            productmetadf = ct_total_df.select(
                "product_id", "category_code", "brand"
            ).distinct()
            product_catalog_df = productcountdf.join(
                productavgdf, ["product_id", "main_category"]
            ).join(productmetadf, "product_id")
            if self.save_flag:
                product_catalog_df.write.option("overwrite", "true").partitionBy(
                    "main_category"
                ).parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_by_category_store/"
                    + self.save_tag
                    + "/"
                )
            ct_total_df.unpersist()

def main():
  execution_mode="user_profile_etl"
  raw_data_files = [
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2019-Nov.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2019-Oct.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2019_Dec.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Jan.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Feb.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Mar.csv",\
      "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Apr.csv"
      ]
  output_parquet_save_path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/"
  year_month={"2020":["03","04"]}
  paths=[]

  if execution_mode=="raw_data_etl":
    for path in raw_data_files:
      RawDataProcessor(path,output_parquet_save_path)
  else:
    for year, months in year_month.items():
      for month in months:
        paths.append("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/date="+year+"-"+month+"*")
    basePath="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/"
    tdf=spark.read.option("basePath",basePath).parquet(*paths)
    
    if execution_mode=="user_profile_etl":
      UserProfileGenerator(tdf=tdf,save_flag=True,save_tag="2",category_flag=True)
    elif execution_mode=="catalog_etl":
      CatalogGenerator(tdf=tdf,save_flag=True,save_tag="4",category_flag=True)

if __name__ == "__main__":
    main()