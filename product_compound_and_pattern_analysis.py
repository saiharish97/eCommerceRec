# -*- coding: utf-8 -*-
"""product_compound_and_pattern_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sMHmCol7DrtREF6KUul-61aQvpQtz3HK

#Description of the analytics developed in this notebook
Product Compound Analytics 
- Product Conversion rates
- Revenue per View
- User product view history

Product Purchase Pattern Analytics
- Approach #1 : associative confidence score between 2 products based on how many users/user_sessions for viewed/purchased events.
- Approach #2 : Market Basket Analysis through spark ML model.

# Spark Setup and Initialization
"""

from google.colab import drive
drive.mount('/content/drive')
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf /content/drive/Shareddrives/FourYottaBytes_DA231o/spark-3.0.3-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop2.7"

import findspark
findspark.init()
findspark.find()
from pyspark.sql import SparkSession
spark = SparkSession.builder\
         .master("local[*]")\
         .appName("Colab")\
         .config('spark.ui.port', '4050')\
         .getOrCreate()
spark

"""# Import Raw Data Files"""

### IMPORTS ###
import math
import os,sys
import findspark
import pandas as pd
import statistics as st
from typing import Any
from google.colab import drive
from pyspark.sql.functions import sum, col, split, expr, concat, concat_ws, lit, to_date, avg, max, countDistinct, avg, min, stddev, udf, round
from pyspark.sql.types import FloatType, TimestampType, IntegerType

### CONFIG ###

year_month={"2019":["10","11","12"], "2020":["01","02","03","04"]}
#year_month={"2019":["10"]}
raw_data_files = [#"/content/drive/Shareddrives/FourYottaBytes_DA231o/2019-Nov.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2019-Oct.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2019_Dec.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Jan.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Feb.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Mar.csv",\
                  "/content/drive/Shareddrives/FourYottaBytes_DA231o/2020-Apr.csv"]
output_parquet_save_path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/"

#Sample parquet read for selected year and months
dfp=[]
for year, months in year_month.items():
  for month in months:
    path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/date="+year+"-"+month+"*"
    print("Reading :"+path)
    df=spark.read.parquet(path)
    dfp.append(df)
    

total_df=dfp[0]
for i in range(1,len(dfp)):
  total_df=total_df.union(dfp[i])

#***To read the whole data***#
#total_df=spark.read.parquet("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/")

total_df.printSchema()

"""#Import catalog and user history compound Analysis Files"""

from pyspark.sql import SparkSession
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col,collect_list,udf,avg,max,min,when,rank,lit
from pyspark.sql.types import IntegerType, StructType, StructField, StringType, FloatType

catalog_path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_store/"
user_history_path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_store/"
dur=60
historical_data={}
historical_data["catalog"]=spark.read.parquet(catalog_path+str(math.floor(dur/30))+"/")
historical_data["user_history"]=spark.read.parquet(user_history_path+str(math.floor(dur/30))+"/")

purchase_catalog=historical_data["catalog"].filter(col("event_type")=="purchase").drop("event_type")
purchase_catalog.cache()
historical_data["user_history"]=spark.read.parquet(user_history_path+str(math.floor(dur/30))+"/")

"""# Product Compund Analytics
[To be merged to user_history and catalog stat generator]
"""

import pyspark.sql.functions as F
from pyspark.sql.functions import desc

"""### Product Conversion Rate
   to be added to catalog stat generator
"""

purchase_events=total_df.filter(col("event_type")=="purchase").select("product_id").groupBy("product_id").agg(F.count("product_id").alias("total_sales")).filter(col("total_sales")>1000)
#purchase_events.show(10)
view_events=total_df.filter(col("event_type")=="view").select("product_id").groupBy("product_id").agg(F.count("product_id").alias("total_views")).filter(col("total_views")>1000)
Prod_conversion_rate = purchase_events.join(view_events,purchase_events["product_id"] == view_events["product_id"], "inner").withColumn("conv_rate",col('total_sales')/col('total_views')).sort(desc("conv_rate")).drop("product_id")
Prod_conversion_rate.show(10) #full total_df takes 21 mins

"""### product revenue per view
  to be added to catalog
"""

sales_events=total_df.filter(col("event_type")=="purchase").select("product_id","price").withColumn("total_sales", F.count("product_id").over((Window.partitionBy('product_id')))).filter(col("total_sales")>1000)
#sales_events.show(10)
revenue_events=sales_events.groupBy("product_id").agg(F.sum("price").alias("total_revenue"))
#revenue_events.show(10)
view_events=total_df.filter(col("event_type")=="view").select("product_id").groupBy("product_id").agg(F.count("product_id").alias("total_views")).filter(col("total_views")>1000)
Prod_revenue_rate = revenue_events.join(view_events,revenue_events["product_id"] == view_events["product_id"], "inner").withColumn("revenue_rate",col('total_revenue')/col('total_views')).sort(desc("revenue_rate"))

"""###user product view history & product co-view confidence score
 to be added to user_history
"""

import itertools
from pyspark.sql import functions as F
from pyspark.sql.functions import size,explode, collect_set, array_distinct

combinations_udf = F.udf(
    lambda x: list(itertools.combinations(x, 2)), "array<array<string>>" 
)
#can get product association based on how many users(user_id) or how many user_session
target_col = "user_id"
#makes more sense to find common products in a user_session
#target_col = "user_session"

user_product_view_history = total_df.filter(col("event_type") == "view").select(target_col, "product_id").groupBy(target_col).agg(collect_set("product_id").alias("product_view_history"))

# (prod1,prod2) viewed by how many users
product_combinations = user_product_view_history.withColumn("new_col", combinations_udf(F.col("product_view_history"))).filter(size(F.col("new_col"))> 0).drop(target_col,"product_view_history")
flatmap_prod_comb = product_combinations.select(explode(F.col("new_col"))).withColumn("prod1",F.col('col')[0]).withColumn("prod2",F.col('col')[1]).groupBy("prod1","prod2").count() # 5.33 mins for 2 months
#flatmap_prod_comb.show(10)

#prod1 viewed by how many users 
product_count = total_df.filter(col("event_type") == "view").groupBy("product_id").agg(size(collect_set(target_col)).alias("product_count")).filter(F.col("product_count")>=10)

#get the association rule with confidence
product_view_association_rule = product_count.join(flatmap_prod_comb,col("product_id") == col("prod1")).withColumn("confidence", F.col("count")/F.col("product_count")).orderBy(col("confidence").desc()) #drop("product_count","count")
product_view_association_rule.show(10)

product_view_association_rule.count()

"""###user product purchase history & product co-purchase confidence score"""

user_product_purchase_history=total_df.filter(col("event_type") == "purchase").select(target_col, "product_id").groupBy(target_col).agg(collect_set("product_id").alias("uniq_prod_history"))

# (prod1,prod2) viewed by how many users
product_pur_combinations = user_product_purchase_history.withColumn("new_col", combinations_udf(F.col("uniq_prod_history"))).filter(size(F.col("new_col"))> 0).drop(target_col,"uniq_prod_history")
flatmap_prod_pur_comb = product_pur_combinations.select(explode(F.col("new_col"))).withColumn("prod1",F.col('col')[0]).withColumn("prod2",F.col('col')[1]).groupBy("prod1","prod2").count() # 5.33 mins for 2 months
#flatmap_prod_comb.show(10)

#prod1 viewed by how many users 
product_count = total_df.filter(col("event_type") == "purchase").groupBy("product_id").agg(size(collect_set(target_col)).alias("product_count")).filter(F.col("product_count")>=10)

#get the association rule with confidence
product_purchase_association_rule = product_count.join(flatmap_prod_pur_comb,col("product_id") == col("prod1")).withColumn("confidence", F.col("count")/F.col("product_count")).orderBy(col("confidence").desc()) #drop("product_count","count")
product_purchase_association_rule.show(10)

product_purchase_association_rule.count()

"""#Product Purchase Pattern Analytics

Market basket analysis to reveal product groupings and products that are likely to be purchased together.

[To be added as new api in the homepage class]
"""

from pyspark.ml.fpm import FPGrowth
from pyspark.sql.functions import array_distinct

#can get product association based on how many users(user_id) or how many user_session
target_col = "user_id"
#makes more sense to find common products in a user_session
#target_col = "user_session"

"""###Prepare the dataframe to train the spark ML model"""

#We use the frequent pattern mining supported in spark ML called the FP-growth algorithm to perform the market basket analysis.

#To train the model, we will prepare the first the needed data into a dataframe.
#We intend to find the association rules between various product purchases from all users, 
#so we are going to analyse market baskets regarding product purchases per user id.
#so data needed for it - userId, product purchases

#The user history compound analysis has the data for userId and list of product purchases.
#Perform the data cleaning - 
#1.Each userId is unique with regards user_history dictionary.
#2.Maintain the events with event_type=purchase only. 
#2.Each product id in the list corresponding to the userId should also be unique

#user_history_df=historical_data["user_history"].select("user_id","product_history").filter(col("event_type")=="purchase")
#user_history_uniq = user_history_df.withColumn("uniq_prod_history",array_distinct("product_history")).drop("product_history")
user_product_purchase_history=total_df.filter(col("event_type") == "purchase").select(target_col, "product_id").groupBy(target_col).agg(collect_set("product_id").alias("uniq_prod_history"))

"""###Train the FPGrowth model choosing appropriate hyperparameters"""

#provide the hyperparameters for the model - 
# minSupport - support the frequency of purchase of the product. All product with atleast minSupport are considered frequent by the model.
#minConfidence - Hyperparameter to specify the minimum confidence for generating association rules from frequent products.
#Note : The dataset has many product ids, but the support 2 or more products is very low. Confidence scores not shown for minsupport > 0.001
minS = 0.001
minC = 0.0001
fpGrowth = FPGrowth(itemsCol="uniq_prod_history", minSupport=minS, minConfidence=minC)

#Train the model on the prepared dataframe containing the itemset.
model = fpGrowth.fit(user_product_purchase_history)

"""###Get the frequent products and the association rules from the model"""

# Get the frequent product identified by the model
model.freqItemsets.orderBy(col("freq").desc())

#Display generated association rules with confidence scores
#the output is a dataframe containing confidence above minConfidence,
#antecedent = numerator of the ratio for confidence, consequent = denominator of the ratio.
model.associationRules.orderBy(col("confidence").desc())

model.freqItemsets.write.parquet("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/FrequentPattern/FreqItemSets/"+target_col+"/all_months.parquet")
model.associationRules.write.parquet("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/FrequentPattern/associationRules/"+target_col+"/all_months.parquet")

#TBD - prediction

"""#Product Pattern File Generation"""

def product_pattern_gen(total_df, target_col, minS, minC ):
  #We use the frequent pattern mining supported in spark ML called the FP-growth algorithm to perform the market basket analysis.

  #To train the model, we will prepare the first the needed data into a dataframe.
  #We intend to find the association rules between various product purchases from all users, 
  #so we are going to analyse market baskets regarding product purchases per user id.
  #so data needed for it - userId, product purchases

  #The user history compound analysis has the data for userId and list of product purchases.
  #Perform the data cleaning - 
  #1.Each userId is unique with regards user_history dictionary.
  #2.Maintain the events with event_type=purchase only. 
  #2.Each product id in the list corresponding to the userId should also be unique

  #user_history_df=historical_data["user_history"].select("user_id","product_history").filter(col("event_type")=="purchase")
  #user_history_uniq = user_history_df.withColumn("uniq_prod_history",array_distinct("product_history")).drop("product_history")
  user_product_purchase_history=total_df.filter(col("event_type") == "purchase").select(target_col, "product_id").groupBy(target_col).agg(collect_set("product_id").alias("uniq_prod_history"))

  #provide the hyperparameters for the model - 
  # minSupport - support the frequency of purchase of the product. All product with atleast minSupport are considered frequent by the model.
  #minConfidence - Hyperparameter to specify the minimum confidence for generating association rules from frequent products.
  #Note : The dataset has many product ids, but the support 2 or more products is very low. Confidence scores not shown for minsupport > 0.001
  
  fpGrowth = FPGrowth(itemsCol="uniq_prod_history", minSupport=minS, minConfidence=minC)

  #Train the model on the prepared dataframe containing the itemset.
  model = fpGrowth.fit(user_product_purchase_history)

  print("Writing: /content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/FrequentPattern_"+year+"_"+month+".parquet")
  model.freqItemsets.write.parquet("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/FrequentPattern/FreqItemSets/"+target_col+"/"+year+"_"+month+".parquet")
  model.associationRules.write.parquet("/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/FrequentPattern/associationRules/"+target_col+"/"+year+"_"+month+".parquet")

year_month={"2019":["10","11","12"], "2020":["01","02","03","04"]}
for year, months in year_month.items():
  for month in months:
    path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/date="+year+"-"+month+"*"
    print("Reading :"+path)
    df=spark.read.parquet(path)
    target_col = "user_id" #user_session
    minS = 0.001
    minC = 0.0001
    product_pattern_gen(df,"user_id",minS,minC)

"""#API to Query

###API for homepage
[parameters to be defined]
"""

#Get_frequent_products(top=, #product_in_group=)
model.freqItemsets.orderBy(col("freq").desc()).show(10)
#Get_product_confidence(productId)
model.associationRules.orderBy(col("confidence").desc()).show(10)

"""#Plots

#[Not related to project] python/pyspark coding
"""

import itertools
from pyspark.sql import functions as F

combinations_udf = F.udf(
    lambda x: list(itertools.combinations(x, 2)), "array<array<string>>" 
)

  
df = spark.createDataFrame([("u1",['hot','summer', 'book'],), 
                            ("u2",['g', 'o', 'p'], ),
                           ], ['userid', 'col1'])

df1 = df.withColumn("new_col", combinations_udf(F.col("col1")))
df1.show()
