# -*- coding: utf-8 -*-
"""StatGenerator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z9sRZFvP7nIU3vZVvxOQNZ7a4smFhku4
"""

from google.colab import drive
drive.mount('/content/drive/',force_remount=True)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf /content/drive/Shareddrives/FourYottaBytes_DA231o/spark-3.0.3-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop2.7"
import findspark
findspark.init()
findspark.find()
from pyspark.sql import SparkSession
spark = SparkSession.builder\
         .master("local[*]")\
         .appName("Colab")\
         .config('spark.ui.port', '4050')\
         .getOrCreate()
spark

### IMPORTS ###
import math
import pandas as pd
from pyspark.sql.functions import col,collect_list,udf,avg,stddev_pop,split
from pyspark.sql.types import IntegerType, StructType, StructField, StringType, FloatType
import statistics as st

from typing import Any
class StatGenerator:
    total_df = Any
    save_flag = False
    save_tag = "new"
    category_flag = False

    def __init__(self, tdf, save_flag, save_tag, category_flag):
        self.total_df = tdf
        self.save_flag = save_flag
        self.save_tag = save_tag
        self.category_flag = category_flag

    def user_history_generator(self):
        if self.category_flag == False:
            self.total_df.cache()
            countdf = (
                self.total_df.select("user_id", "event_type")
                .groupBy("user_id", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            avgdf = (
                self.total_df.select("user_id", "event_type", "price")
                .groupBy("user_id", "event_type")
                .avg("price")
                .withColumn("avg_event_price", col("avg(price)"))
                .drop("avg(price)")
            )
            event_history = (
                self.total_df.select("user_id", "event_type", "price")
                .groupBy("user_id", "event_type")
                .agg(
                    stddev_pop("price").alias("stddev"),    # PATHURI,VIEW,[10,20,1111,121212,1,22] [#1,#777,#17298127]
                    collect_list("price").alias("event_history"),
                )
            )
            user_product_history = (
                self.total_df.filter(col("event_type") == "purchase")
                .select("user_id", "product_id")
                .groupBy("user_id")
                .agg(collect_list("product_id").alias("product_history"))
            )
            user_history_df = (
                countdf.join(avgdf, ["user_id", "event_type"])
                .join(event_history, ["user_id", "event_type"])
                .join(user_product_history, "user_id")
            )
            if self.save_flag:
                user_history_df.write.option("overwrite", "true").parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_store/"
                    + self.save_tag
                    + "/"
                )
            self.total_df.unpersist()
        else:
            ct_total_df = self.total_df.withColumn(
                "main_category", split(col("category_code"), "\.")[0]
            )
            ct_total_df.cache()
            countdf = (
                ct_total_df.select("user_id", "main_category", "event_type")
                .groupBy("user_id", "main_category", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            avgdf = (
                ct_total_df.select("user_id", "main_category", "event_type", "price")
                .groupBy("user_id", "main_category", "event_type")
                .avg("price")
                .withColumn("avg_event_price", col("avg(price)"))
                .drop("avg(price)")
            )
            event_history = (
                ct_total_df.select("user_id", "main_category", "event_type", "price")
                .groupBy("user_id", "main_category", "event_type")
                .agg(
                    stddev_pop("price").alias("stddev"),
                    collect_list("price").alias("event_history"),
                )
            )
            user_product_history = (
                ct_total_df.filter(col("event_type") == "purchase")
                .select("user_id", "main_category", "product_id")
                .groupBy("user_id", "main_category")
                .agg(collect_list("product_id").alias("product_history"))
            )
            user_history_df = (
                countdf.join(avgdf, ["user_id", "main_category", "event_type"])
                .join(event_history, ["user_id", "main_category", "event_type"])
                .join(user_product_history, ["user_id", "main_category"])
            )
            if self.save_flag:
                user_history_df.write.option("overwrite", "true").partitionBy(
                    "main_category"
                ).parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_by_category_store/"
                    + self.save_tag
                    + "/"
                )
            ct_total_df.unpersist()

    def product_catalog_generator(self):
        if self.category_flag == False:
            self.total_df.cache()
            productcountdf = (
                self.total_df.select("product_id", "event_type")
                .groupBy("product_id", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            productavgdf = (
                self.total_df.select("product_id", "price")
                .groupBy("product_id")
                .avg("price")
                .withColumn("avg_price", col("avg(price)"))
                .drop("avg(price)")
            )
            productmetadf = self.total_df.select(
                "product_id", "category_code", "brand"
            ).distinct()
            product_catalog_df = productcountdf.join(productavgdf, "product_id").join(
                productmetadf, "product_id"
            )
            if self.save_flag:
                product_catalog_df.write.option("overwrite", "true").parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_store/"
                    + self.save_tag
                    + "/"
                )
            self.total_df.unpersist()
        else:
            ct_total_df = self.total_df.withColumn(
                "main_category", split(col("category_code"), "\.")[0]
            )
            ct_total_df.cache()
            productcountdf = (
                ct_total_df.select("product_id", "main_category", "event_type")
                .groupBy("product_id", "main_category", "event_type")
                .count()
                .withColumn("event_count", col("count"))
                .drop("count")
            )
            productavgdf = (
                ct_total_df.select("product_id", "main_category", "price")
                .groupBy("product_id", "main_category")
                .avg("price")
                .withColumn("avg_price", col("avg(price)"))
                .drop("avg(price)")
            )
            productmetadf = ct_total_df.select(
                "product_id", "category_code", "brand"
            ).distinct()
            product_catalog_df = productcountdf.join(
                productavgdf, ["product_id", "main_category"]
            ).join(productmetadf, "product_id")
            if self.save_flag:
                product_catalog_df.write.option("overwrite", "true").partitionBy(
                    "main_category"
                ).parquet(
                    "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_by_category_store/"
                    + self.save_tag
                    + "/"
                )
            ct_total_df.unpersist()

#Sample parquet read for selected year and months
def main():
  dfp=[]
  #year_month={"2020":["01","02","03","04"]}
  year_month={"2020":["01","02","03","04"]}
  for year, months in year_month.items():
    for month in months:
      path="/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/schema_verified/date="+year+"-"+month+"*"
      print("Reading :"+path)
      df=spark.read.parquet(path)
      dfp.append(df)

  tdf=dfp[0]
  for i in range(1,len(dfp)):
    tdf=tdf.union(dfp[i])
  
  stg=StatGenerator(tdf=tdf,save_flag=True,save_tag="4",category_flag=True)
  stg.user_history_generator()
  stg.product_catalog_generator()
  stg=StatGenerator(tdf=tdf,save_flag=True,save_tag="4",category_flag=False)
  stg.user_history_generator()
  stg.product_catalog_generator()

if __name__ == "__main__":
    main()

drive.mount('/content/drive/',force_remount=True)





user_history_df.filter(col("event_type")=="purchase").filter(col("event_count")>=4).show(20,0)

user_history_df.filter(col("event_type")=="view").filter(col("event_count")>=70).show(20,0)