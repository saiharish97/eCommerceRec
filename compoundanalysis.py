# -*- coding: utf-8 -*-
"""CompoundAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1geZdV1qQRiHnX4TmlV4cW8owSAf44_D6
"""

### IMPORTS ###
import math
import os, sys
import findspark
import pandas as pd
import statistics as st
from typing import Any
from google.colab import drive
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col, collect_list, udf, avg, max, min, when, rank, lit
from pyspark.sql.types import (
    IntegerType,
    StructType,
    StructField,
    StringType,
    FloatType,
)


class Homepage:
    user_id = []
    topK = 0
    duration = 0
    purchase_thresholds = 0
    view_thresholds = 0
    catalog_path = ""
    user_history_path = ""
    historical_data = {}
    spark = None

    def __init__(self, id, dur, purchase_threshold, view_threshold, topk, spark):
        self.user_id = id
        self.duration = dur
        self.purchase_thresholds = purchase_threshold
        self.view_thresholds = view_threshold
        self.topK = topk
        self.spark = spark
        self.catalog_path = "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/catalog_store/"
        self.user_history_path = "/content/drive/Shareddrives/FourYottaBytes_DA231o/eCommerce/compoundAnalysisResources/user_history_store/"
        try:
            if dur < 30:
                raise ValueError(
                    "Got duration less than 30. Please check and pass the value between [30, 190]"
                )
            else:
                self.historical_data["catalog"] = spark.read.parquet(
                    self.catalog_path + str(math.floor(dur / 30)) + "/"
                )
                self.historical_data["user_history"] = spark.read.parquet(
                    self.user_history_path + str(math.floor(dur / 30)) + "/"
                )
        except ValueError as val_error:
            print("Caught this error: " + repr(val_error))

    def get_recommendations_by_price(self):
        user_history_df = self.historical_data["user_history"].filter(
            col("user_id").isin(self.user_id)
        )
        user_history_df.cache()

        purchase_user_history = user_history_df.filter(
            col("event_type") == "purchase"
        ).filter(col("event_count") >= self.purchase_thresholds)
        purchase_users = [
            r[0] for r in purchase_user_history.select("user_id").rdd.collect()
        ]
        view_users = [item for item in self.user_id if item not in purchase_users]
        view_user_history = (
            user_history_df.filter(col("user_id").isin(view_users))
            .filter(col("event_type") == "view")
            .filter(col("event_count") >= self.view_thresholds)
        )
        view_users = [r[0] for r in view_user_history.select("user_id").rdd.collect()]
        cold_start_users = [
            item
            for item in self.user_id
            if (item not in purchase_users and item not in view_users)
        ]

        purchase_catalog = (
            self.historical_data["catalog"]
            .filter(col("event_type") == "purchase")
            .drop("event_type")
        )
        purchase_catalog.cache()

        user_action_df = purchase_user_history.select(
            "user_id",
            "event_type",
            "event_count",
            "avg_event_price",
            "stddev",
            "product_history",
        ).union(
            view_user_history.select(
                "user_id",
                "event_type",
                "event_count",
                "avg_event_price",
                "stddev",
                "product_history",
            )
        )
        user_action_with_bounds_df = (
            user_action_df.withColumn(
                "sub_fac",
                when(col("stddev") <= 20, 40)
                .when(col("stddev") / 2 <= 20, col("stddev"))
                .otherwise(col("stddev") / 2),
            )
            .withColumn("lower_bound", col("avg_event_price") - col("sub_fac"))
            .withColumn("upper_bound", col("avg_event_price") + col("sub_fac"))
            .drop("sub_fac")
        )
        user_bound_info = user_action_with_bounds_df.select(
            "user_id",
            "event_type",
            "avg_event_price",
            "stddev",
            "product_history",
            "lower_bound",
            "upper_bound",
        )
        user_prod_join = (
            user_bound_info.join(
                purchase_catalog,
                [
                    col("avg_price") >= col("lower_bound"),
                    col("avg_price") <= col("upper_bound"),
                ],
            )
            .filter("!array_contains(product_history, product_id)")
            .drop("product_history")
        )
        window = Window.partitionBy(col("user_id")).orderBy(col("event_count").desc())
        action_rec_df = user_prod_join.select(
            "*", rank().over(window).alias("rank")
        ).filter(col("rank") <= self.topK)
        cold_start_rec = (
            purchase_catalog.orderBy("event_count", ascending=False)
            .select("product_id", "category_code", "brand", "avg_price")
            .limit(self.topK)
            .withColumn("users", lit(str(cold_start_users)))
        )

        user_history_df.unpersist()
        purchase_catalog.unpersist()

        return (action_rec_df, cold_start_rec)


def setup_env(flag):
    if flag:
        os.system("apt-get install openjdk-8-jdk-headless -qq > /dev/null")
        os.system(
            "tar xf /content/drive/Shareddrives/FourYottaBytes_DA231o/spark-3.0.3-bin-hadoop2.7.tgz"
        )
        os.system("pip install -q findspark")
    os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
    os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop2.7"
    findspark.init()
    findspark.find()
    drive.mount("/content/drive")
    spark = (
        SparkSession.builder.master("local[*]")
        .appName("Colab")
        .config("spark.ui.port", "4050")
        .getOrCreate()
    )
    return spark


def main():
    spark = Any
    setup_arg = sys.argv[1:][0]
    if setup_arg == "setup":
        spark = setup_env(True)
    else:
        spark = setup_env(False)
    print("This is the Homepage Recommendation Script:")
    h_obj = Homepage(
        id=[
            "512389317",
            "513696407",
            "514688413",
            "494701812",
            "512571292",
            "coldster1",
            "coldster2",
            "628167977",
        ],
        dur=30,
        purchase_threshold=4,
        view_threshold=70,
        topk=4,
        spark=spark,
    )
    recs = h_obj.get_recommendations_by_price()
    recs[0].show(100, 0)
    recs[1].show(100, 0)


if __name__ == "__main__":
    main()
